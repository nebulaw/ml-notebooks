{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38a00a2c-21b7-42b0-930f-528c51854d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x72374f5381f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(42069)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcd9435b-0167-4ea2-9d54-5ebf0d4869a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32\n",
    "block_size = 32 # this is context length\n",
    "max_iters = 4000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384//2\n",
    "n_layer = 6\n",
    "n_head = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "251fda6d-6fd5-4d6e-a6da-13fb68d1f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read galaktioni poems\n",
    "with open(\"../data/gala.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "    text = text.replace(\"\\xad\", \"-\")\n",
    "\n",
    "\n",
    "# prepare vocabulary, encode and decode functions\n",
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "stoi = { s:i for i, s in enumerate(vocab) }\n",
    "itos = { i:s for s, i in stoi.items() }\n",
    "encode = lambda seq: [stoi[ch] for ch in seq]\n",
    "decode = lambda key: ''.join([itos[i] for i in key])\n",
    "\n",
    "\n",
    "# encode text\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "# split dataset\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c23c723-e9de-4baf-89ab-62b83e16147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random batching function\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95377cc8-d845-4eaa-af94-b0ae47dc3a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        # computing attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd, n_embd), # projection layer\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffn = FeedForward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.sa(x)\n",
    "        # x = self.ffn(x)\n",
    "        # added residual connections for better optimization\n",
    "        x = x + self.sa(x) \n",
    "        x = x + self.ffn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# create bigram model\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
    "        # self.ffn = FeedForward(n_embd)\n",
    "        self.decoder_blocks = nn.Sequential(*[DecoderBlock(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.layer_norm = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_embd = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_embd = self.position_embedding_table(torch.arange(T, device=device)) # (B, T, vocab_size)\n",
    "        x = tok_embd + pos_embd\n",
    "        # x = self.sa_heads(x) # one head of attention\n",
    "        # x = self.ffn(x)\n",
    "        x = self.decoder_blocks(x)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.lm_head(x) # decoder\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T, C)\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get preds\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only the last time step\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax for probs\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e08bf560-6a3a-411b-8131-f5ad7f096567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our bigram model\n",
    "model = GPTLanguageModel()\n",
    "# send parameters to device\n",
    "m = model.to(device)\n",
    "# you are a nice optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82d9091a-bf4a-4880-9518-69278cfc1033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of iter=100, loss: 2.3630013465881348\n",
      "end of iter=200, loss: 2.3230140209198\n",
      "end of iter=300, loss: 2.364772081375122\n",
      "end of iter=400, loss: 2.215888261795044\n",
      "on step    500, train loss is 2.1366 and eval loss is 2.2176\n",
      "end of iter=500, loss: 2.269651412963867\n",
      "end of iter=600, loss: 2.191009044647217\n",
      "end of iter=700, loss: 2.1270029544830322\n",
      "end of iter=800, loss: 2.133702516555786\n",
      "end of iter=900, loss: 2.09895396232605\n",
      "on step   1000, train loss is 1.9765 and eval loss is 2.1061\n",
      "end of iter=1000, loss: 2.150238513946533\n",
      "end of iter=1100, loss: 2.0744080543518066\n",
      "end of iter=1200, loss: 2.0608341693878174\n",
      "end of iter=1300, loss: 2.0375118255615234\n",
      "end of iter=1400, loss: 1.9807690382003784\n",
      "on step   1500, train loss is 1.8765 and eval loss is 2.0406\n",
      "end of iter=1500, loss: 1.9520326852798462\n",
      "end of iter=1600, loss: 1.91845703125\n",
      "end of iter=1700, loss: 1.9669588804244995\n",
      "end of iter=1800, loss: 1.9760539531707764\n",
      "end of iter=1900, loss: 1.8822541236877441\n",
      "on step   2000, train loss is 1.7936 and eval loss is 2.0101\n",
      "end of iter=2000, loss: 2.0021276473999023\n",
      "end of iter=2100, loss: 1.8722243309020996\n",
      "end of iter=2200, loss: 1.9006924629211426\n",
      "end of iter=2300, loss: 1.908980369567871\n",
      "end of iter=2400, loss: 1.9264696836471558\n",
      "on step   2500, train loss is 1.7415 and eval loss is 1.9603\n",
      "end of iter=2500, loss: 1.888024926185608\n",
      "end of iter=2600, loss: 1.9352061748504639\n",
      "end of iter=2700, loss: 1.8123668432235718\n",
      "end of iter=2800, loss: 1.9092787504196167\n",
      "end of iter=2900, loss: 1.854201316833496\n",
      "on step   3000, train loss is 1.6963 and eval loss is 1.9530\n",
      "end of iter=3000, loss: 1.8460177183151245\n",
      "end of iter=3100, loss: 1.8181875944137573\n",
      "end of iter=3200, loss: 1.741202712059021\n",
      "end of iter=3300, loss: 1.7358311414718628\n",
      "end of iter=3400, loss: 1.7743648290634155\n",
      "on step   3500, train loss is 1.6634 and eval loss is 1.9181\n",
      "end of iter=3500, loss: 1.790444016456604\n",
      "end of iter=3600, loss: 1.7812843322753906\n",
      "end of iter=3700, loss: 1.76576828956604\n",
      "end of iter=3800, loss: 1.769592046737671\n",
      "end of iter=3900, loss: 1.734633445739746\n"
     ]
    }
   ],
   "source": [
    "# train phase\n",
    "for iter in range(1, max_iters):\n",
    "    # every once in a while evaluate on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"on step {iter:-6}, train loss is {losses[\"train\"]:.4f} and eval loss is {losses[\"val\"]:.4f}\")\n",
    "\n",
    "    # sample random batch\n",
    "    Xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # evaluate model\n",
    "    logits, loss = model(Xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iter % 100 == 0: print(f\"end of {iter=}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ab2e62-612e-4030-83d1-a56efb827d20",
   "metadata": {},
   "source": [
    "### Note\n",
    "Generated output really really depends on the hyperparameters. Currently,\n",
    "the model was trained on the smallest possible hyperparamers due to\n",
    "my laptop performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b541c282-3fc7-4a9f-9415-e5dd6777c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "მე ღმერთივით მთელი ლამავსას ადროს ოღოთავს ნაცვლებდა გეშეხატული ორვალთ,\n",
      "როღაცა გაჩრდა, ყოფნოება\n",
      "- წყალს ღელ ტყა, ვინაც ვერ გარწმივალი,\n",
      "ჰაეროდა დამძლურს... მაშ..\n",
      "არ თერი, მოგძინურ შუქი ადღოთა ყვავით\n",
      "ლად სილისკერა წამწარე ღამე ვნება ყოს ხმაში ირგვლს;\n",
      "ვისეს შევებსჭვავს ეხედა, სიჩნევა ახვს;\n",
      "ყოფნა ნაზი სევდის კვლავ და კიცხა გშენს არ განაღვარს?\n",
      "\n",
      "წაე, წამინდა ყოფნისას: არსაწმას თვალება\n",
      "ან ათებს მწარვალეს გამიტევიცხურ შური\n",
      "მართ გულინდი დაატომათქვევს შავბნელი,\n",
      "როსომ ვიგრძებში გავხოვებულ სიყვარულთ შორეს შუ\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102bf8b2-0ccc-463b-ab15-147b0793695c",
   "metadata": {},
   "source": [
    "This is completely a random b*lshit in Georgian"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
